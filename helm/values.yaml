# Default values for InferaDB Engine Helm chart

# Image configuration
image:
  repository: inferadb-engine
  pullPolicy: IfNotPresent
  tag: "" # Overrides the image tag (default is Chart appVersion)

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Replica configuration
replicaCount: 3

# Service account
serviceAccount:
  create: true
  annotations: {}
  name: ""
  automount: false

# Pod annotations
podAnnotations:
  prometheus.io/scrape: "true"
  prometheus.io/port: "8080"
  prometheus.io/path: "/metrics"

# Pod security context
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 65532
  runAsGroup: 65532
  fsGroup: 65532
  seccompProfile:
    type: RuntimeDefault

# Container security context
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 65532

# Service configuration
service:
  type: ClusterIP
  port: 8080
  grpcPort: 8081
  meshPort: 8082 # Service mesh API for inter-service communication
  annotations: {}
  sessionAffinity: None
  # nodePort: 30080  # Only used when service.type is NodePort
  # grpcNodePort: 30081
  # meshNodePort: 30082
  # loadBalancerIP: ""  # Only used when service.type is LoadBalancer
  headless:
    enabled: false # Enable for StatefulSet deployments

# Ingress configuration
ingress:
  enabled: false
  className: ""
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # cert-manager.io/cluster-issuer: letsencrypt-prod
  hosts:
    - host: inferadb.local
      paths:
        - path: /
          pathType: Prefix
  tls: []
  #  - secretName: inferadb-tls
  #    hosts:
  #      - inferadb.local

# Resource limits
resources:
  requests:
    cpu: 500m
    memory: 512Mi
  limits:
    cpu: 2000m
    memory: 2Gi

# Horizontal Pod Autoscaling
autoscaling:
  enabled: true
  minReplicas: 3
  maxReplicas: 20
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80
  customMetrics: []
    # - type: Pods
    #   pods:
    #     metric:
    #       name: inferadb_requests_per_second
    #     target:
    #       type: AverageValue
    #       averageValue: "1000"

# Pod Disruption Budget
podDisruptionBudget:
  enabled: true
  minAvailable: 2

# Node selector
nodeSelector: {}

# Tolerations
tolerations: []

# Affinity rules
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchExpressions:
              - key: app.kubernetes.io/name
                operator: In
                values:
                  - inferadb-engine
          topologyKey: kubernetes.io/hostname

# Topology spread constraints
topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: topology.kubernetes.io/zone
    whenUnsatisfiable: ScheduleAnyway
    labelSelector:
      matchLabels:
        app.kubernetes.io/name: inferadb-engine

# InferaDB Engine configuration
config:
  # Worker threads for async runtime (default: number of CPU cores)
  threads: 4
  # Log level: trace, debug, info, warn, error
  logging: "info"

  # Storage backend: "memory" or "ledger"
  storage: "memory"

  # Ledger configuration (only used when storage = "ledger")
  ledger:
    endpoint: "http://inferadb-ledger-client:50051"
    clientId: ""
    namespaceId: ""

  # Cache configuration
  cache:
    enabled: true
    capacity: 10000 # Max cache entries
    ttl: 300 # TTL in seconds

  # Token validation configuration
  token:
    cacheTtl: 300 # JWKS cache TTL in seconds
    clockSkew: 60 # Clock skew tolerance in seconds
    maxAge: 86400 # Maximum token age in seconds (24 hours)

  # Mesh configuration (communication with Control service)
  mesh:
    url: "" # Set automatically via discovery, or set to static URL (e.g., "http://inferadb-control:9092")
    timeout: 5000 # Timeout for mesh API calls in milliseconds
    cacheTtl: 300 # Org/vault cache TTL in seconds
    certCacheTtl: 900 # Client cert cache TTL in seconds

  # Optional: provide full config file as YAML string
  configFile: ""

# Service discovery configuration
discovery:
  # Discovery mode: "none", "kubernetes", "tailscale"
  mode: "kubernetes"

  # Cache TTL for discovered endpoints (in seconds)
  cacheTtl: 300

  # Health check interval (in seconds)
  healthCheckInterval: 30

  # Control service discovery (used to construct mesh.url when discovery is enabled)
  control:
    # Kubernetes service name (when mode = kubernetes)
    serviceName: "inferadb-control"
    namespace: "inferadb"
    port: 9092 # Control mesh API port

    # Static URL (when mode = none, set config.mesh.url directly instead)
    staticUrl: "http://localhost:9092"

  # Tailscale multi-region configuration (when mode = tailscale)
  tailscale:
    # Enable Tailscale sidecar
    enabled: false

    # Local cluster name (e.g., "us-west-1")
    localCluster: "primary"

    # Remote clusters to discover via Tailscale mesh
    # remoteClusters:
    # - name: "eu-west-1"
    #   tailscaleDomain: "eu-west-1.ts.net"
    #   serviceName: "inferadb-engine"
    #   port: 8081  # gRPC port for replication
    #   regionId: "eu-west-1"  # Optional: region ID for replication topology (defaults to name)
    # - name: "ap-southeast-1"
    #   tailscaleDomain: "ap-southeast-1.ts.net"
    #   serviceName: "inferadb-engine"
    #   port: 8081
    remoteClusters: []

    # Tailscale auth key (reference to secret)
    authKey:
      # Use existing secret
      existingSecret: "tailscale-auth"
      key: "authkey"
      # Or provide directly (not recommended for production)
      value: ""

    # Tailscale sidecar image
    image:
      repository: tailscale/tailscale
      tag: "v1.56.1"
      pullPolicy: IfNotPresent

    # Resource limits for Tailscale sidecar
    resources:
      requests:
        cpu: 50m
        memory: 64Mi
      limits:
        cpu: 200m
        memory: 128Mi

    # Additional Tailscale arguments
    extraArgs: "--accept-routes"

    # Accept DNS from Tailscale
    acceptDNS: false

    # Advertise routes
    advertiseRoutes: ""

# Multi-region replication configuration
replication:
  # Enable replication
  enabled: false

  # Replication strategy: "ActiveActive", "PrimaryReplica", "MultiMaster"
  strategy: "ActiveActive"

  # Local region identifier (must match discovery.tailscale.localCluster for Tailscale mode)
  localRegion: "" # Defaults to discovery.tailscale.localCluster

  # Conflict resolution strategy: "LastWriteWins", "SourcePriority", "InsertWins"
  conflictResolution: "LastWriteWins"

  # Source priority order (for SourcePriority strategy)
  sourcePriority: []
    # - us-west-1
    # - eu-central-1
    # - ap-southeast-1

  # Agent configuration
  agent:
    # Maximum retry attempts for replication
    maxRetries: 5
    # Delay between retries in milliseconds
    retryDelayMs: 100
    # Batch size for replication batches
    batchSize: 100

  # Region configuration (alternative to Tailscale discovery)
  # Use this for static region configuration or when not using Tailscale
  regions: []
    # - id: "us-west-1"
    #   name: "US West"
    #   isPrimary: true  # Only for PrimaryReplica strategy
    #   zones:
    #     - id: "us-west-1a"
    #       name: "Zone A"
    #       nodes:
    #         - id: "node1"
    #           endpoint: "engine-us-west-1:8081"

  # Replication targets (which regions replicate to which)
  # For ActiveActive: auto-generated (all to all)
  # For PrimaryReplica: primary replicates to all replicas
  # For MultiMaster: specify custom targets
  replicationTargets: {}
    # us-west-1:
    #   - eu-central-1
    #   - ap-southeast-1

# Secrets (use external secret manager in production)
secrets:
  # Engine identity Ed25519 private key (PEM format) for signing requests to Control
  # If not provided, will be generated on startup (not recommended for production)
  pem: ""


  # Additional custom secrets
  additional: {}

# External Secrets Operator integration
externalSecrets:
  enabled: false
  secretStoreRef:
    name: ""
    kind: SecretStore # or ClusterSecretStore
  refreshInterval: 1h
  data: []
    # - secretKey: INFERADB__ENGINE__PEM
    #   remoteRef:
    #     key: inferadb/prod/engine
    #     property: pem

# Health check configuration
healthCheck:
  liveness:
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    successThreshold: 1
    failureThreshold: 3

  readiness:
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 3

  startup:
    initialDelaySeconds: 0
    periodSeconds: 5
    timeoutSeconds: 3
    successThreshold: 1
    failureThreshold: 30

# Graceful shutdown
terminationGracePeriodSeconds: 30

# Init containers (e.g., wait for Ledger)
initContainers: []
  # - name: wait-for-ledger
  #   image: fullstorydev/grpcurl:v1.8.9
  #   command:
  #   - sh
  #   - -c
  #   - |
  #     until grpcurl -plaintext inferadb-ledger-client:50051 grpc.health.v1.Health/Check; do
  #       sleep 2
  #     done

# Extra environment variables
extraEnv: []
  # - name: CUSTOM_VAR
  #   value: "custom-value"

# Extra volume mounts
extraVolumeMounts: []
  # - name: extra-config
  #   mountPath: /extra
  #   readOnly: true

# Extra volumes
extraVolumes: []
  # - name: extra-config
  #   configMap:
  #     name: extra-config

# ServiceMonitor for Prometheus Operator
serviceMonitor:
  enabled: false
  interval: 30s
  scrapeTimeout: 10s
  labels: {}
  honorLabels: true
  relabelings: []
  metricRelabelings: []

# External dependencies
ledger:
  enabled: false
  serviceName: "inferadb-ledger-client"
  port: 50051
