#!/usr/bin/env python3
"""
Analyze test execution times from nextest JUnit XML output.

This script parses JUnit XML files generated by cargo-nextest and:
- Identifies tests exceeding configurable time thresholds
- Compares against baseline timings to detect regressions
- Supports updating baselines for intentional changes

Usage:
    # Analyze test times (default: target/nextest/ci/junit.xml)
    python scripts/analyze-test-times.py

    # Analyze specific JUnit file
    python scripts/analyze-test-times.py --junit target/nextest/fast/junit-fast.xml

    # Update baseline with current timings
    python scripts/analyze-test-times.py --update-baseline

    # Set custom thresholds
    python scripts/analyze-test-times.py --slow-threshold 2.0 --regression-threshold 30

Exit codes:
    0 - All tests within thresholds
    1 - Tests exceed time budget or regressions detected
    2 - Error (file not found, parse error, etc.)
"""

import argparse
import json
import sys
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from pathlib import Path
from typing import Optional


# Default paths relative to repository root
DEFAULT_JUNIT_PATHS = [
    "target/nextest/ci/junit.xml",
    "target/nextest/fast/junit-fast.xml",
    "target/nextest/default/junit.xml",
]
DEFAULT_BASELINE_PATH = ".config/test-baselines.json"

# Default thresholds
DEFAULT_SLOW_THRESHOLD_SECONDS = 1.0
DEFAULT_REGRESSION_THRESHOLD_PERCENT = 20


@dataclass
class TestResult:
    """Represents a single test's execution result."""

    name: str
    classname: str
    time_seconds: float

    @property
    def full_name(self) -> str:
        """Returns fully qualified test name."""
        return f"{self.classname}::{self.name}"


@dataclass
class AnalysisResult:
    """Results of test time analysis."""

    total_tests: int
    total_time_seconds: float
    slow_tests: list[TestResult]
    regressions: list[tuple[TestResult, float, float]]  # (test, current, baseline)
    improvements: list[tuple[TestResult, float, float]]  # (test, current, baseline)


def parse_junit_xml(junit_path: Path) -> list[TestResult]:
    """Parse JUnit XML file and extract test results."""
    if not junit_path.exists():
        raise FileNotFoundError(f"JUnit XML file not found: {junit_path}")

    tree = ET.parse(junit_path)
    root = tree.getroot()

    results = []
    for testsuite in root.findall(".//testsuite"):
        for testcase in testsuite.findall("testcase"):
            name = testcase.get("name", "unknown")
            classname = testcase.get("classname", "unknown")
            time_str = testcase.get("time", "0")

            try:
                time_seconds = float(time_str)
            except ValueError:
                time_seconds = 0.0

            results.append(TestResult(name=name, classname=classname, time_seconds=time_seconds))

    return results


def load_baseline(baseline_path: Path) -> dict[str, float]:
    """Load baseline timings from JSON file."""
    if not baseline_path.exists():
        return {}

    with open(baseline_path) as f:
        data = json.load(f)
        return data.get("tests", {})


def save_baseline(baseline_path: Path, test_times: dict[str, float]) -> None:
    """Save test timings as baseline."""
    baseline_path.parent.mkdir(parents=True, exist_ok=True)

    data = {
        "version": 1,
        "description": "Test execution time baselines for regression detection",
        "tests": test_times,
    }

    with open(baseline_path, "w") as f:
        json.dump(data, f, indent=2, sort_keys=True)
        f.write("\n")


def analyze_tests(
    results: list[TestResult],
    baseline: dict[str, float],
    slow_threshold: float,
    regression_threshold: float,
) -> AnalysisResult:
    """Analyze test results against thresholds and baseline."""
    slow_tests = []
    regressions = []
    improvements = []

    for test in results:
        # Check if test is slow
        if test.time_seconds > slow_threshold:
            slow_tests.append(test)

        # Check for regression/improvement against baseline
        if test.full_name in baseline:
            baseline_time = baseline[test.full_name]
            if baseline_time > 0:
                percent_change = ((test.time_seconds - baseline_time) / baseline_time) * 100

                if percent_change > regression_threshold:
                    regressions.append((test, test.time_seconds, baseline_time))
                elif percent_change < -regression_threshold:
                    improvements.append((test, test.time_seconds, baseline_time))

    # Sort by time (slowest first)
    slow_tests.sort(key=lambda t: t.time_seconds, reverse=True)
    regressions.sort(key=lambda t: t[1], reverse=True)
    improvements.sort(key=lambda t: t[2] - t[1], reverse=True)

    return AnalysisResult(
        total_tests=len(results),
        total_time_seconds=sum(t.time_seconds for t in results),
        slow_tests=slow_tests,
        regressions=regressions,
        improvements=improvements,
    )


def print_analysis(analysis: AnalysisResult, slow_threshold: float, regression_threshold: float) -> None:
    """Print analysis results in a human-readable format."""
    print(f"\n{'=' * 60}")
    print("TEST PERFORMANCE ANALYSIS")
    print(f"{'=' * 60}\n")

    print(f"Total tests analyzed: {analysis.total_tests}")
    print(f"Total execution time: {analysis.total_time_seconds:.2f}s")
    print(f"Slow threshold: {slow_threshold}s")
    print(f"Regression threshold: {regression_threshold}%")

    # Slow tests
    print(f"\n{'-' * 60}")
    print(f"SLOW TESTS (>{slow_threshold}s): {len(analysis.slow_tests)}")
    print(f"{'-' * 60}")

    if analysis.slow_tests:
        for test in analysis.slow_tests[:20]:  # Top 20
            print(f"  {test.time_seconds:6.2f}s  {test.full_name}")
        if len(analysis.slow_tests) > 20:
            print(f"  ... and {len(analysis.slow_tests) - 20} more")
    else:
        print("  None - all tests within threshold")

    # Regressions
    print(f"\n{'-' * 60}")
    print(f"REGRESSIONS (>{regression_threshold}% slower): {len(analysis.regressions)}")
    print(f"{'-' * 60}")

    if analysis.regressions:
        for test, current, baseline in analysis.regressions[:10]:
            percent = ((current - baseline) / baseline) * 100
            print(f"  {current:6.2f}s (was {baseline:.2f}s, +{percent:.0f}%)  {test.full_name}")
        if len(analysis.regressions) > 10:
            print(f"  ... and {len(analysis.regressions) - 10} more")
    else:
        print("  None - no significant regressions detected")

    # Improvements (informational)
    if analysis.improvements:
        print(f"\n{'-' * 60}")
        print(f"IMPROVEMENTS (>{regression_threshold}% faster): {len(analysis.improvements)}")
        print(f"{'-' * 60}")

        for test, current, baseline in analysis.improvements[:5]:
            percent = ((baseline - current) / baseline) * 100
            print(f"  {current:6.2f}s (was {baseline:.2f}s, -{percent:.0f}%)  {test.full_name}")
        if len(analysis.improvements) > 5:
            print(f"  ... and {len(analysis.improvements) - 5} more")

    print()


def find_junit_file(specified_path: Optional[str]) -> Path:
    """Find the JUnit XML file to analyze."""
    if specified_path:
        return Path(specified_path)

    for path_str in DEFAULT_JUNIT_PATHS:
        path = Path(path_str)
        if path.exists():
            return path

    raise FileNotFoundError(
        f"No JUnit XML file found. Tried: {', '.join(DEFAULT_JUNIT_PATHS)}\n"
        "Run 'cargo nextest run --profile ci' first to generate test results."
    )


def main() -> int:
    """Main entry point."""
    parser = argparse.ArgumentParser(
        description="Analyze test execution times from nextest JUnit XML output.",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    parser.add_argument(
        "--junit",
        type=str,
        help="Path to JUnit XML file (default: auto-detect)",
    )
    parser.add_argument(
        "--baseline",
        type=str,
        default=DEFAULT_BASELINE_PATH,
        help=f"Path to baseline JSON file (default: {DEFAULT_BASELINE_PATH})",
    )
    parser.add_argument(
        "--slow-threshold",
        type=float,
        default=DEFAULT_SLOW_THRESHOLD_SECONDS,
        help=f"Threshold in seconds for slow tests (default: {DEFAULT_SLOW_THRESHOLD_SECONDS})",
    )
    parser.add_argument(
        "--regression-threshold",
        type=float,
        default=DEFAULT_REGRESSION_THRESHOLD_PERCENT,
        help=f"Threshold percentage for regression detection (default: {DEFAULT_REGRESSION_THRESHOLD_PERCENT})",
    )
    parser.add_argument(
        "--update-baseline",
        action="store_true",
        help="Update baseline with current test timings",
    )
    parser.add_argument(
        "--fail-on-slow",
        action="store_true",
        default=True,
        help="Exit with error if slow tests detected (default: true)",
    )
    parser.add_argument(
        "--no-fail-on-slow",
        action="store_false",
        dest="fail_on_slow",
        help="Don't exit with error for slow tests (only regressions)",
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress informational output",
    )

    args = parser.parse_args()

    try:
        # Find and parse JUnit file
        junit_path = find_junit_file(args.junit)
        if not args.quiet:
            print(f"Analyzing: {junit_path}")

        results = parse_junit_xml(junit_path)

        if not results:
            print("No test results found in JUnit XML.")
            return 2

        # Update baseline if requested
        if args.update_baseline:
            baseline_path = Path(args.baseline)
            test_times = {t.full_name: t.time_seconds for t in results}
            save_baseline(baseline_path, test_times)
            print(f"Baseline updated: {baseline_path} ({len(test_times)} tests)")
            return 0

        # Load baseline and analyze
        baseline = load_baseline(Path(args.baseline))
        analysis = analyze_tests(
            results,
            baseline,
            args.slow_threshold,
            args.regression_threshold,
        )

        # Print results
        if not args.quiet:
            print_analysis(analysis, args.slow_threshold, args.regression_threshold)

        # Determine exit code
        has_regressions = len(analysis.regressions) > 0
        has_slow_tests = len(analysis.slow_tests) > 0 and args.fail_on_slow

        if has_regressions:
            print(f"ERROR: {len(analysis.regressions)} test(s) have regressed significantly.")
            print("Run with --update-baseline to accept new timings.")
            return 1

        if has_slow_tests:
            print(f"WARNING: {len(analysis.slow_tests)} test(s) exceed {args.slow_threshold}s threshold.")
            # Don't fail for slow tests by default in CI - just warn
            # Only fail if there are regressions against baseline

        return 0

    except FileNotFoundError as e:
        print(f"ERROR: {e}", file=sys.stderr)
        return 2
    except ET.ParseError as e:
        print(f"ERROR: Failed to parse JUnit XML: {e}", file=sys.stderr)
        return 2
    except json.JSONDecodeError as e:
        print(f"ERROR: Failed to parse baseline JSON: {e}", file=sys.stderr)
        return 2


if __name__ == "__main__":
    sys.exit(main())
